---
title: "Actividad 3: Modelización predictiva"
author: "Iván García Jiménez, Itziar Ricondo"
date: "5 de enero de 2021"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
    number_sections: yes
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
---

******
# Tareas a realizar en la práctica
******

Siguiendo las principales etapas de un proyecto analítico, las diferentes tareas a realizar (y
justificar) son las siguientes:

**1. Descripción del dataset. ¿Por qué es importante y qué pregunta/problema pretende responder?**

**2. Integración y selección de los datos de interés a analizar.**

**3. Limpieza de los datos.**

  3.1. ¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno
de estos casos?

  3.2. Identificación y tratamiento de valores extremos.

**4. Análisis de los datos.**

  4.1. Selección de los grupos de datos que se quieren analizar/comparar (planificación
de los análisis a aplicar).

  4.2. Comprobación de la normalidad y homogeneidad de la varianza.

  4.3. Aplicación de pruebas estadísticas para comparar los grupos de datos. En función
de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis,
correlaciones, regresiones, etc. Aplicar al menos tres métodos de análisis
diferentes.

**5. Representación de los resultados a partir de tablas y gráficas.**

**6. Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema?**

**7. Código: Hay que adjuntar el código, preferiblemente en R, con el que se ha realizado la limpieza, análisis y representación de los datos. Si lo preferís, también podéis trabajar en Python.**


******
# Descripción del dataset. 
******
¿Por qué es importante y qué pregunta/problema pretende responder?

## Sobre el conjunto de datos

Este conjunto de datos contiene datos de uso de camiones de gran tonelaje de Scania. El caso de estudio se centra en el Sistema de Aire a Presión (APS, del inglés *Air Pressure System*), sistema responsable de proveer de aire comprimido a varias de las funciones del camión, como el freno o embrague. Los casos con clase positiva son fallos de componentes relacionados con el sistema APS. Por el contrario, los caos con clase negativa corresponden a fallos de componentes no relacionados con el APS. El conjunto de datos presenta casos de ambos tipos, si bien es cierto que los casos negativos son mucho más frecuentes.

El objetivo del conjunto de datos es predecir qué tipo de fallos tienen su origen en el sistema APS y cuáles son debidos a otras causas. Este hecho tiene repercusión en las revisiones o reparaciones a realizar sobre el camión. De hecho, la bondad de la predicción se medirá en términos de coste. El coste total se evaluará como la suma de fallos de tipo 1 por el coste asociado (Coste1) más la suma de fallos de tipo 2 por el coste asociado (Coste2).Un error de tipo I se origina cuando el modelo predice positivo cuando en realidad el fallo no tiene origen en un componente el APS. El coste 1 asociado a este hecho es debido a la revisión innecesaria del APS que debe ser realizada en el taller, que se ha valorado en 10 unidades. Por el contrario, el error de tipo II podría tener como resultado permitir el tráfico del camión con defecto en un componente APS, que puede causar una averís. El coste 2 asociado a este tipo de fallo es superior y se ha valorado en 500 unidades. El mejor modelo de predicción será aquel que minimice el coste total.

Este conjunto ha sido proporcionado por la empresa Scania y están disponibles en el repositorio UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks). Los datos proporcionados son un conjunto de datos de entrenamiento y otro de validación.

Este conjunto de datos trata sobre la predicción de fallos. Se ha seleccionado este conjunto de datos porque la identificación del estado de componentes (desgaste, fallo, vida remanente) es un tema de gran relevancia dentro de las iniciativas de Industria 4.0 y, en concreto, de técnicas de análisis de datos, minería y aprendizaje automático. 

Por otro lado, este conjunto de datos presenta una serie de retos para los autores de este análisis, que se listan a continuación:

-  Gran número de variables y además las variables están anonimizadas.  El conjunto de datos de entrenamiento presenta 171 variables y 60000 observaciones. El hecho de que se hayan recodificado (anonimizado) las variables impide tener información física adicional que ayude en la interpretación. Sí se menciona el hecho de que algunas variables son atributos y otras representan histogramas. El número tan elevado de variables implica que **se tendrán que utilizar herramientas que permitan automatizar el tratamiento de los datos**, que supone un hecho diferencial sobre el tipo de tratamientos realizados hasta ahora por nosotros.

- Los casos positivos y negativos están desequilibrados. Deberá analizarse qué implica este hecho en el análisis de los datos.

- El conjunto de datos tiene un fuerte carácter industrial. El resultado debe ayudar en la toma de decisiones desde una perspectiva de coste.


******
# Integración y selección de los datos de interés a analizar
******

## Lectura del archivo de datos:

El dataset está compuesto por 2 archivos: aps_failure_training_set.csv que contiene los datos para entrenar el modelo, y aps_failure_test_set.csv que contiene los datos para evaluarlo.

Debido a la gran cantidad de datos del dataset, se ha optado por leerlos mediante la función "fread"" de la libreria "data.table", ya que esta función permite una lectura de datos más rápida que con la función "read.csv". 

En la descripción del dataset de UCI Machine Learning Repositoy, se nos informa de antemano que el dataset contiene valores nulos (perdidos) asignados como "na", lo indicamos por parámetro en la función fread para que capte de manera correcta los valores perdidos.

```{r}
if (!require("data.table")) install.packages("data.table")
library(data.table)
# Lectura de los dataset de train y test
url_train <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00421/aps_failure_training_set.csv"
url_test <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00421/aps_failure_test_set.csv"
train <- fread(url_train, na.strings = "na", strip.white = TRUE)
test <- fread(url_test, na.strings = "na", strip.white = TRUE)
# Dimensiones del dataset
dim(train)
```

Podemos ver que el conjunto de entrenamiento contiene una gran cantidad de datos, 60000 registros y 171 atributos; a continuación vemos su estructura.

```{r}
# Estructura del dataset
str(train)
```

## Consideraciones iniciales sobre la estructura de los datos

Los datos son leídos como chart, hay que pasar a numeric.
La primera variable del dataset es la clase, es decir, el valor a predecir. El resto de las variables son numéricas, pero algunas se han asignado como integer64, para evitar errores se ha decidido transformar las variables de tipo integer64 a tipo entero.

```{r}
if (!require("dplyr")) install.packages("dplyr")
library(dplyr)
is.integer64 <- function(x){class(x)=="integer64"}
train <- train %>% mutate_if(is.integer64, as.integer)
test <- test %>% mutate_if(is.integer64, as.integer)
```

Nos interesa ver cómo se distribuye la variable de clasificación, esto influye considerablemente tanto en la limpieza a realizar como en su análisis posterior.

```{r}
# visualizamos la distribución de la variable class
library(ggplot2)
qplot(as.factor(train$class), xlab = "class")
```

Podemos observar un claro desequilibrio entre clases, tenemos muchos más camiones que han dado negativo que camiones que han dado positivo. Este desequilibrio es algo que se debe tener muy en cuenta al momento de tratar con este dataset.

Se hace un screening de datos mediante las funciones "ExpData"de la librería "SmartEDA"  y "skim" de la libreria "skimr". Se ha decidido utilizar estas funciones en vez de "summary", ya que al tener grandes cantidades de datos, es más visual ver un atributo por fila tal y como vemos a continuación:

```{r}
if (!require("SmartEDA")) install.packages("SmartEDA")
library(SmartEDA)
library(skimr)
## EDA, Datos faltantes, ceros y outliers
ExpData(data=train,type=1)
eda_skim<-skim(train)
eda_skim
```


******
# Limpieza de los datos
******

Se puede apreciar cómo muchos de los atributos tienes ceros hasta el tercer cuartil, esto es algo que se debe revisar para entender a qué se debe y cómo gestionarlo. También hay valores atributos que presentan una baja completitud, para tratar los datos nulos (missing values) comenzamos por visualizar el porcentage de valores nulos de cada atributo; los mostramos en orden descendente para poder ver claramente qué atributos contienen mayor porcentaje de valores nulos.

## Gestión de ceros y datos perdidos

```{r}
# Estadísticas de valores vacíos
sort(colMeans(is.na(train)), decreasing = TRUE)
```

Se puede ver que hay atributos con más de un 20% de valores perdidos, por lo tanto se ha decicido eliminar estos atributos y trabajar únicamente con el resto.

```{r}
columns_to_remove <- which(colMeans(is.na(train)) > 0.2)
train_filtered = subset(train, select = -c(columns_to_remove) )
dim(train_filtered)
```

El resto de valores nulos deberíamos imputarlos o eliminarlos, pero antes hay una duda que resolver, igual que hay columnas con un alto porcentaje de valores perdidos, es posible que también haya filas con un alto porcentaje de valores perdidos

```{r}
sort(rowMeans(is.na(train_filtered)), decreasing = TRUE)
```

Podemos ver que hay filas en las que el porcentaje de nulos es superior al 50%, es decir que desconocemos la mayoría de valores de esta fila. Veamos si el hecho de desconocer estos valores influye en que la clase sea negativa o positiva.

```{r}
null_rows <- filter(train_filtered, rowMeans(is.na(train_filtered)) > 0.5)
qplot(as.factor(null_rows$class), xlab = "class")
```

Podemos ver cómo la distribución obtenida para estos datos es muy parecida a la del total de los datos, por lo tanto, el hecho de desconocer la mayoría de valores de una fila no influye en que la clase sea negativa o positiva. Esto permite eliminar estos registros de nuestro data set.

```{r}
train <- filter(train_filtered, rowMeans(is.na(train_filtered)) < 0.5)
dim(train)
```

El resto de valores perdidos se han imputado con la mediana para no afectar a la distribución de los atributos.

```{r}
library(imputeMissings)
train <- impute(train, object = NULL, method = "median/mode", flag = FALSE)
eda_skim<-skim(train)
eda_skim
```

Una vez tratados los valores perdidos, se procede al tratamiento de ceros. Como se ha visto antes hay atributos en los que hay ceros hasta en el tercer cuartil, vamos a investigar a qué se debe. En la descripción del dataset, se indica que hay 7 variables que estan binarizadas en 10 partes, es decir, que hay atributos que ocupan 10 columnas. Seleccionamos los atributos binarizados.

```{r}
histogram <- train[,c(6:15, 32:41, 42:51, 52:61, 88:97, 100:109, 136:145)]
dim(histogram)
```

Para entender que información contiene estos atributos binarizados se mostrará uno de ellos para el primer camión del dataset.

```{r}
histogram_1 <- as.numeric(histogram[1, c(1:10)])
barplot(histogram_1)
```

Si se suman todos los valores del atributo se obtiene el número total de sucesos. 

```{r}
sum(histogram_1)
```

Al hacer la suma en otro atributo también se obtiene el mismo número total de sucesos.

```{r}
histogram_2 <- as.numeric(histogram[1, c(11:20)])
sum(histogram_2)
```

Esto nos hace deducir, que la suma de todos los valores del histograma representa la cantidad de tiempo que el camión ha estado encendido a lo largo de su vida.

Si por ejemplo un atributo es la temperatura, y se divide en 10 intérvalos ([-20º,-10º], [-10º, 0º], ... , [70º, 80º] ), tendremos la cantidad de tiempo (por ejemplo minutos) que el camión ha estado en cada intérvalo de temperatura a lo largo de su vida. Al sumar los valores de todos los intérvalos, se obtiene entonces el tiempo total que el camión ha estado encendido a lo largo de su vida.

Por eso la suma, en todos los atributos da siempre el mismo resultado, porque cada atributo se ha monitorizado durante el tiempo que el camión está encendido.

Esto nos hace pensar que los ceros en los histogramas son valores que aportan información útil, y no se deben eliminar o imputar. Ya que si por ejemplo un camión ha estado mucho tiempo en temperaturas extremas, puede afectar gravemente al sistema APS, pero si nunca ha estado en temperaturas extremas, puede que el sistema APS esté en buen estado.

Pasamos a tratar los ceros de los atributos no binarizados.

```{r}
other <- select(train, -colnames(histogram))
sort(colMeans(other=="0"), decreasing = TRUE)
```

Se puede apreciar que algunos atributos no binarizados tienen más de un 90% de ceros (en concreto hay 10 atributos que tienen más de un 99% de ceros). Procedemos a eliminar los atributos con más de un 90% de ceros.

```{r}
columns_to_remove <- which(colMeans(other=="0") > 0.9)
train = subset(train, select = -c(columns_to_remove) )
dim(train)
```

Se visualizan los valores únicos antes de normalizar.

```{r}
# Ver valores únicos
apply(train, 2, function(x) length(unique(x)))
```

Se puede ver como el atributo cd_000 contiene un único valor para el data set, por lo tanto desechamos este atributo.

```{r}
train <- subset(train, select=-c(cd_000))
dim(train)
```

En el dataframe de test, nos quedaremos con las mismas columnas que en el de train y también imputaremos los nulos con la mediana.

```{r}
test <- select(test, colnames(train))
test <- impute(test, object = NULL, method = "median/mode", flag = FALSE)
dim(test)
```

Se seleccionan los datos numéricos y se normalizan. Después de la normalización se vuelve a hacer un screening de datos.

```{r}
train_X.scaled <- scale(train[,c(2:129)])
test_X.scaled <- scale(test[,c(2:129)])
eda_skim<-skim(train_X.scaled)
eda_skim
```

Con los datos normalizados se hace una reducción de dimensionalidad mediante PCA.

```{r}
train.pca <- prcomp(train_X.scaled[,c(1:128)], center = TRUE, scale = TRUE)
test.pca <- prcomp(test_X.scaled[,c(1:128)], center = TRUE, scale = TRUE)
summary(data.pca) 
```

Una vez obtenidos los componenentes principales, se calcula la explicación de varianza de cada componente principal.

```{r}
pct_var_explained <- train.pca$sdev^2 / sum(train.pca$sdev^2)
cumsum(pct_var_explained)
```

Nos quedamos únicamente con aquellos componentes que explican un 95% de la varianza.

```{r}
train <- data.frame(
    class = train$class, 
    predict(train.pca)[, cumsum(pct_var_explained) < 0.95]
)

dim(train)
```

Seleccionamos los mismos coponentes principales para el conjunto de test.

```{r}
test <- data.frame(
    class = test$class, 
    predict(test.pca)[, cumsum(pct_var_explained) < 0.95]
)

dim(test)
```

******
# Análisis de los datos
******

# Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar).

Después de realizar PCA, nos hemos quedado con 66 atributos, separamos la clase a predecir del resto de datos.

```{r}
trainx <- train[,2:66]
trainy <- as.factor(train[,1])
testx <- test[,2:66]
testy <- as.factor(test[,1])
```

El objetivo de este conjunto de datos es la clasificación de fallo con origen en el APS. Para ello se implementarán las siguientes técnicas:

* árbol de decisión con el algoritmo c5.0.

* regresión logística (no realizada todavía)

# Comprobación de la normalidad y homogeneidad de la varianza

En la práctica se pide comprobar la normalidad de de los datos. POr ejemplo, no se puede utilizar la función shapiro.test porque el tamaño de la muestra debe ser de un máximo de 5000. Se utiliza la función normality() del paquete dlookr, que realiza el test de Shapiro-Wilks a todas las variables numéricas. Si el tamaño de la muestra es superior a 5000, como es el caso, la función realiza una muestra de tamaño 5000 para calcular el estadístico. Las variables de trainx tienen un valor de significancia menor a 0.05, por lo que se rechaza la normalidad de la distribución. Se utiliza la función plot_normality (para las 5 primeras variables) para visualizar el resultado de normalidad. En el QQPlot se observa la falta de normalidad, con una pendiente casi horizontal en la parte derecha de los residuos

```{r}
if (!require("dlookr")) install.packages("dlookr")
library(dlookr)
normality_test<-normality(trainx)
head(normality_test,5)  # No hace caso al head, no muestra los resultados
# Visualization of all numerical variables
plot_normality(trainx[,1:5])
plot(trainx[,1])
plot(trainx[,2])
plot(trainx[,3])
plot(trainx[,4])
plot(trainx[,5])
plot(trainx[,1:5])

```

# Árbol de decisión con el algoritmo c5.0.

```{r}
model <- C50::C5.0(trainx, trainy,rules=TRUE )
summary(model)
```

Se ha generado un árbol que tiene 50 reglas de desición y a priori solo tiene un error del 0.6%

```{r}
model <- C50::C5.0(trainx, trainy)
plot(model)
```

Predecimos la clase para los vaores de test y calculamos la precisión del árbol.

```{r}
predicted_model <- predict( model, testx, type="class" )
print(sprintf("La precisión del árbol es: %.4f %%",100*sum(predicted_model == testy) / length(predicted_model)))
```

Generamos la matriz de confusión para encontrar mostrar los falsos positivos y los falsos negativos.

```{r}
mat_conf<-table(testy,Predicted=predicted_model)
mat_conf
```

Calculamos el coste de la predicción, los falsos positivos tienen un coste valorado en 10 unidades, en cambio, los falsos negativos tienen un coste valorado en 500 unidades. 

```{r}
coste_total <- 180*10 + 251*500
coste_total
```

El coste total ha sido de 127300 unidades, trataremos de perfeccionar nuestro análisis para reducir este coste, sobre todo nos centraremos en reducir los falsos negativos, ya que generan un coste 50 veces mayor.

******
# Conclusiones
******